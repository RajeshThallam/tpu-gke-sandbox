# Maxtext examples

This folder contains example  configurations for pretraining  [Maxtext LLM](https://github.com/google/maxtext) using [C4 Dataset](https://www.tensorflow.org/datasets/catalog/c4). 

The `single_slice` folder is a Kustomize **overlay** over the **base** Job specification of a single slice training workload. The `multi_slice` folder is an **overlay** over the **base** JobSet specification of a multi-slice training workload.

Before configuring and running Maxtext training jobs you need to need download the C4 dataset and build a training container image with the Maxtext code base.

## Download C4 Dataset

Set PROJECT_ID and GCS_BUCKET to your project ID and a GCS bucket created during setup.

```
PROJECT_ID=jk-mlops-dev
GCS_BUCKET=gs://jk-gke-aiml-repository
DATASET_LOCATION="$GCS_BUCKET/datasets/c4/en/3.0.1"

gsutil -u $PROJECT_ID -m cp gs://allennlp-tensorflow-datasets/c4/en/3.0.1/* $DATASET_LOCATION

```

## Build Maxtext training container image

The `build.yaml` file is a **Cloud Build** configuration that automates a process of packaging the Maxtext code base into a docker container image and pushing it to your project's **Container Registry**

```
CLOUD_BUILD_REGION=us-central1
MAXTEXT_IMAGE_NAME=maxtext-runner-image

gcloud builds submit \
--project $PROJECT_ID \
--region $CLOUD_BUILD_REGION \
--substitutions _MAXTEXT_IMAGE_NAME=$MAXTEXT_IMAGE_NAME \
--config build.yaml \
--machine-type=e2-highcpu-32
```

## Training parameters



## Run a single slice training job

Modify the `single_slice/job-spec-patch.yaml` file to reflect your environment. At minimum, modify the followining fields:
- Set the `metadata.name` field with to a unique job name. Although not mandatory, using a unique name for each job helps with managing multiple Job resources
- Update the `spec.template.spec.containers[name=tpu-job].image` field with your training image name
- Update the Maxtext trainer parameters defined in the `spec.template.spec.containers[name=tpu-job].command` field. For the detailed information on how to configure Maxtext training runs refer to [Maxtext github repo](https://github.com/google/maxtext/tree/main). At minimum, set the following parameters:
  - `run_name`. This is an identifier of your run. It is used to locate and store artifacts (including checkpoints) generated during training. They will be stored in the `run_name` folder in the `base_output_directory` path (see below). If there is an existing checkpoint in this location that checkpoint will auto-resume.    
  - `base_output_directory`. This a base GCS path for storing artifacts generated during runs.
  - `dataset_path`. This is the GCS location of the C4 dataset. This should be a GCS URI up to but not including the `c4` folder.
  - `steps`. The number of steps for this training run. If you do not set it the default (as defined in `MaxText/configs/base.yml`) is 150,000.

After updating the `configs/job-spec-patch.yaml` you can submit the job using the following command:

```
kubectl apply -k configs
```

You can monitor the job by retrieving logs generated by any worker.

First, list all pods started by the job

```
kubectl get pods -n <YOUR TPU TRAINING NAMESPACE>
```

Pick  any pod in your job and copy its ID. Retrieve the logs for this pod.

```
kubectl logs <YOUR POD ID> -n <YOUR TPU TRAINING NAMESPACE>
```

You can also monitor the job using GCP Console.



Modify the `configs/jobset-spec-patch.yaml` file to reflect your environment. At minimum, modify the followining fields:
- Set the `metadata.name` field with to a unique job name. Although not mandatory, using a unique name for each job helps with managing multiple Job resources
- Update the `spec.template.spec.containers[name=tpu-job].image` field with your training image name
- Update the Maxtext trainer parameters defined in the `spec.template.spec.containers[name=tpu-job].command` field. For the detailed information on how to configure Maxtext training runs refer to [Maxtext github repo](https://github.com/google/maxtext/tree/main). At minimum, set the following parameters:
  - `run_name`. This is an identifier of your run. It is used to locate and store artifacts (including checkpoints) generated during training. They will be stored in the `run_name` folder in the `base_output_directory` path (see below). If there is an existing checkpoint in this location that checkpoint will auto-resume.    
  - `base_output_directory`. This a base GCS path for storing artifacts generated during runs.
  - `dataset_path`. This is the GCS location of the C4 dataset. This should be a GCS URI up to but not including the `c4` folder.
  - `steps`. The number of steps for this training run. If you do not set it the default (as defined in `MaxText/configs/base.yml`) is 150,000.
  - `ici_fsdp_parallelism`. This parameter controls the ICI Fully Sharded Data Parallelism (FSDP) sharding strategy. For this sample it is recommended to set it to a number of chips in a slice.
  - `dcn_data_parallelism`. This parameter controls the DCN Data Parallelism (DP) sharding strategy. For this sample it is recommended to set it to a number of slices.
  - The default configuration in this sample trains a xB parameter model with 16 decoder layers, 8 attention heads per layer, and 2560 model dimension. If you would like train a different model archicture adjust the following parameters: `base_emb_dim, base_num_heads, base_mlp_dim, base_num_decoder_layers, head_dim`
#### Monitoring training metrics using Vertex AI Tensorboard
