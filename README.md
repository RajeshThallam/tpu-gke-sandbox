#  Running TPU training workloads on GKE

This reference guide compiles best practices, prescriptive guidance, and code samples for running large-scale TPU v4 and TPU v5e machine learning training workloads on Google Kubernetes Engine (GKE).
The guide covers two main topics:
- **Configuring a GKE based environment for large scale training on Cloud TPUs**
  - This section describes how to configure a GKE cluster to optimize it for running large-scale machine learning training workloads on [Cloud TPUs](https://cloud.google.com/tpu).
- **Defining, Submitting, and Monitoring Training Jobs**
  - This section provides guidance on how to define, submit, and manage training jobs using the Kubernetes [JobSet](https://github.com/kubernetes-sigs/jobset) and [Kueue](https://github.com/kubernetes-sigs/kueue) APIs.

We also include Terraform configuration for provisioning the training environment and code samples for a variety of training workloads.



## The training environment

The diagram below depicts a high-level architecture of the training environment.


![arch](/images/training-cluster.png)

The foundation of the environment is a regional, VPC-native GKE cluster. The cluster has two types of node pools: a single node pool with CPU-only nodes and several [Multi-host TPU node pools](https://cloud.google.com/kubernetes-engine/docs/concepts/tpus).

This cluster topology supports running both [single-slice and multi-slice TPU](https://cloud.google.com/tpu/docs/multislice-introduction) training jobs.

Training datasets and artifacts produced by training jobs (such as logs and checkpoints) are saved in Google Cloud Storage.

Training, data processing, and other components of a training workload are packaged as Docker container images and managed in Google Cloud Container Registry.

[Vertex AI TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-introduction) is used to track and visualize training metrics.

[Cloud Monitoring](https://cloud.google.com/monitoring) is used to collect and analyze non-functional performance metrics, and [Cloud Logging](https://cloud.google.com/logging) is used to manage logs produced by training workloads.

Training workloads impersonate an Identity and Access Management (IAM) service accounts to access Google Cloud services, such as Google Cloud Storage and Vertex AI TensorBoard.


## Training workload processing 

The following diagram illustrates the process of submitting and processing training workloads in the training environment.

![training workloads](/images/workload-processing.png)

In this guide we advocate using the [Kubernetes JobSet API](https://github.com/kubernetes-sigs/jobset) as the preferred method of coordinating large-scale distributed machine learning training workloads on Kubernetes. When combined with the [Kubernetes Kueue](https://github.com/kubernetes-sigs/kueue) job queuing API, it provides flexible and comprehensive training job orchestration.

The training environment's **Kueue** configuration  consists of a single [ClusterQueue](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/) and multiple [LocalQueues](https://kueue.sigs.k8s.io/docs/concepts/local_queue/). This topology provides basic multi-tenancy and supports managing and prioritizing jobs submitted by multiple teams.

All training workloads are represented as JobSet resources. A JobSet resource may contain multiple job types, such as a core distributed training job and an auxiliary job that manages TensorBoard logs and other artifacts generated by the training job.

JobSet workloads are submitted to a namespaced LocalQueue that points to a ClusterQueue. As illustrated in the diagram, in our reference implementation, there is a single cluster queue.

Kueue monitors when resources (such as TPU slices) required by a workload (JobSet) are available, and then decides when to admit the workload and how to allocate the workload's components to the cluster's node pools. 

For example, a training workload can contain two types of jobs:
- A multi-slice distributed training job
- A job that uploads TensorBoard logs generated by the training job to Vertex AI TensorBoard

When all the resources required by this workload become available, the training job's workers are started on the requested number of TPU slices. The TensorBoard uploader is started on one of the nodes in the CPU node pool.

If the compute resources required by other submitted workloads are not available, these workloads are queued and scheduled for admission based on the priorities that have been defined in the Kueue configuration.

To submit a JobSet-defined workload, you need to create a YAML JobSet resource definition. There are a few different ways to do this. In this guide, we demonstrate two approaches:
- Using [Kustomize](https://kustomize.io/), which helps you create YAML JobSet resource definitions directly.
- Using  [xpk](https://github.com/google/maxtext/tree/main/xpk), which provides an easy-to-use Python-based CLI.


## Environment setup

The provisioning of the environment described in the previous section has been automated with [Terraform](https://cloud.google.com/docs/terraform) and [Cloud Build](https://cloud.google.com/build).

The Terraform configuration can be found in the `env_setup/terraform` folder. Before applying the configuration, you need to select an existing GCP project or create a new one. You will also need to enable the following services:


```
PROJECT_ID=jk-mlops-dev

gcloud config set project $PROJECT_ID

gcloud services enable \
cloudbuild.googleapis.com \
compute.googleapis.com \
cloudresourcemanager.googleapis.com \
iam.googleapis.com \
container.googleapis.com \
cloudtrace.googleapis.com \
iamcredentials.googleapis.com \
monitoring.googleapis.com \
logging.googleapis.com \
aiplatform.googleapis.com \
config.googleapis.com 

```

The following are the tasks performed by the Terraform configuration:
- Creates a network and a subnet for a VPC-native GKE cluster.
- Creates a VPC-native cluster.
- Creates a node pool with nodes equipped with CPUs only.
- Creates a specified number of multi-host TPU node pools.
- Creates an IAM service account for Workload Identity and an IAM service account to be used as a custom node pool service account.
- Assigns a specified set of roles to these service accounts.
- Configures the cluster for Workload Identity.
- Creates a Google Cloud Storage bucket.
- Adds the service accounts to `roles/storage.legacyBucketReader` bucket level permissions.


The Terraform configuration supports the following input variables:

| Variable | Description | Default |
| -------- | ------------|---------|
| region | The compute region for the Cluster | NA|
| tensorboard_region | The compute region for Vertex AI TensorBoard  | NA|
| artifact_repository_bucket_name|The name of the GCS bucket|NA|
| zone | The zone for the TPU node pools. Make sure that the zone supports the required TPU resources| NA |
| networt_name | The name of the network for the cluster | NA |
| subnet_name | The name of the subnet  for the cluster | NA |
| subnet_ip_range | The IP address range for the subnet | 10.129.0.0/20 |
| pods_ip_range | A secondary IP range for pods | 192.168.64.0/20 |
| services_ip_range | A secondary IP range for services | 192.168.80.0/20 |
| cluster_name | The name of the cluster. | NA |
| gke_version | The version of GKE to deploy | 1.27.3-gke.100 |
| cluster_description | The cluster's description | GKE cluster for running TPU training workloads |
| cpu_pool_node_count | The number of nodes in a CPU node pool | 3 |
| cpu_pool_machine_type | The machine type for the CPU node pool | n1-standard-4 |
| cpu_pool_disk_type | The disk type for nodes in the CPU node pool | pd-standard|
| cpu_pool_disk_size | The disk size for noded in the CPU node pool | 200GB |
| tpu_sa_name | The name of the service account that will be provisioned and used for Workload Identity | cloud-tpu-sa |
| tpu_sa_roles | The roles to assign to the service account | roles/storage.objectAdmin, roles/logging.logWriter |
| gke_sa_name | The name of the custom service account for node pools | gke-sa |
| gke_sa_roles | The roles to assigne to the custom service account for node pools | roles/storage.objectAdmin, roles/logging.logWriter |
| tpu_namespace | The K8s namespace for TPU workloads | tpu-training |
| tpu_type | The TPU slice type for TPU node pools. See below for more info | v4-16 |
| num_tpu_pools | The number of multi-host TPU node pools to provision | 1 |
| enable_tpu_autoscaling | Whether to enable outoscaling of TPU node pools | false |
| tpu_node_pool_name_prefix | A prefix that will be used to name TPU node pools. An index starting with 0 will be appended to the prefix to form a TPU node pool name | tpu-node-pool |
| multislice_group_name | A name that will be used to label a TPU node pools to support multi-slice jobs | multi-slice-group |

The `tpu_type` variable is a name of a TPU slice configuration as defined in the following table.

| TPU type name | Slice type | Slice topology | TPU VM type | Number of VMs in a slice | Number of chips in a VM |
| ------------- | -----------|----------------|-------------|--------------------------| ------------------------|
| v5litepod-16 | tpu-v5-lite-podslice | 4x4 | ct5lp-hightpu-4t | 4 | 4 |
| v5litepod-32 | tpu-v5-lite-podslice | 4x8 | ct5lp-hightpu-4t | 8 | 4 |
| v5litepod-64 | tpu-v5-lite-podslice | 8x8 | ct5lp-hightpu-4t | 16 | 4 |
| v5litepod-128 | tpu-v5-lite-podslice | 8x16 | ct5lp-hightpu-4t | 32 | 4 |
| v5litepod-256 | tpu-v5-lite-podslice | 26x16 | ct5lp-hightpu-4t | 64 | 4 |
| v4-8| tpu-v4-podslice | 2x2x1 | ct4p-hightpu-4t | 1 | 4 |
| v4-16| tpu-v4-podslice | 2x2x2 | ct4p-hightpu-4t | 2 | 4 |
| v4-32| tpu-v4-podslice | 2x2x4 | ct4p-hightpu-4t | 4 | 4 |
| v4-64| tpu-v4-podslice | 2x4x4 | ct4p-hightpu-4t | 8 | 4 |
| v4-128| tpu-v4-podslice | 4x4x4 | ct4p-hightpu-4t | 16 | 4 |
| v4-256| tpu-v4-podslice | 4x4x8 | ct4p-hightpu-4t | 32| 4 |
| v4-512| tpu-v4-podslice | 4x8x8 | ct4p-hightpu-4t | 64 | 4 |
| v4-1024| tpu-v4-podslice | 8x8x8 | ct4p-hightpu-4t | 128 | 4 |
| v4-1536| tpu-v4-podslice | 8x8x12 | ct4p-hightpu-4t | 192 | 4 |
| v4-2048| tpu-v4-podslice | 8x8x16 | ct4p-hightpu-4t | 256 | 4 |
| v4-4096| tpu-v4-podslice | 8x16x16 | ct4p-hightpu-4t | 512 | 4 |


The Terraform configuration can be applied directly with the `terraform` CLI. We have also provided a **Cloud Build** configuration - `env_setup/cloudbuild.provision.yaml` - that, in addition to applying the Terraform configuration, installs the **JobSet** and **Kueue** APIs and configures Kueue resources. We recommend using **Cloud Build** to provision and configure the environment in a single step.

Before running **Cloud Build** you need to update the following environment variables to reflect your environment

```
export PROJECT_ID=jk-mlops-dev
export REGION=us-central2
export TENSORBOARD_REGION=us-central1
export ZONE=us-central2-b
export ARTIFACT_REPOSITORY_BUCKET_NAME=jk-gke-aiml-repository
export NETWORK_NAME=jk-gke-network
export SUBNET_NAME=jk-gke-subnet
export CLUSTER_NAME=jk-tpu-training-cluster
export NAMESPACE=tpu-training
export TPU_TYPE=v4-32
export NUM_TPU_POOLS=2
export NUM_OF_CHIPS=32

export JOBSET_API_VERSION="v0.2.3"
export KUEUE_API_VERSION=v0.4.2
```

Note, that we only set a subset of variables exposed by the Terraform configuration. For the other ones we use the defaults. If you want to change the default values of other variables you need to update the `env_setup\cloudbuild.provision.yaml` file and the below `gcloud builds submit` command. 

The Terraform configuration maintains its configuration state in  Google Cloud Storage. Set the following variables to the name of a GCS bucket and a subfolder in the bucket where you want to store the state. 

```
export TF_STATE_BUCKET=jk-mlops-dev-tf-state
export TF_STATE_PREFIX=gke-tpu-training-environment
```

To start provisioning execute the following command:

```
gcloud builds submit \
  --config cloudbuild.provision.yaml \
  --substitutions _TF_STATE_BUCKET=$TF_STATE_BUCKET,_TF_STATE_PREFIX=$TF_STATE_PREFIX,_REGION=$REGION,_TENSORBOARD_REGION=$TENSORBOARD_REGION,_ZONE=$ZONE,_ARTIFACT_REPOSITORY_BUCKET_NAME=$ARTIFACT_REPOSITORY_BUCKET_NAME,_NETWORK_NAME=$NETWORK_NAME,_SUBNET_NAME=$SUBNET_NAME,_CLUSTER_NAME=$CLUSTER_NAME,_NAMESPACE=$NAMESPACE,_TPU_TYPE=$TPU_TYPE,_NUM_TPU_POOLS=$NUM_TPU_POOLS,_NUM_OF_CHIPS=$NUM_OF_CHIPS,_JOBSET_API_VERSION=$JOBSET_API_VERSION,_KUEUE_API_VERSION=$KUEUE_API_VERSION \
  --timeout "2h" \
  --machine-type=e2-highcpu-32 \
  --quiet
```


## Environment clean up

If you want to destroy the environment and clean up all the provisioned resources you can use **Cloud Build** with the `env_setup/cloudbuild.destroy.yaml` configuration.

```
export TF_STATE_BUCKET=jk-mlops-dev-tf-state
export TF_STATE_PREFIX=gke-tpu-training-environment

export PROJECT_ID=jk-mlops-dev
export REGION=us-central2
export TENSORBOARD_REGION=us-central1
export ZONE=us-central2-b
export ARTIFACT_REPOSITORY_BUCKET_NAME=jk-gke-aiml-repository
export NETWORK_NAME=jk-gke-network
export SUBNET_NAME=jk-gke-subnet
export CLUSTER_NAME=jk-tpu-training-cluster
export TPU_TYPE=v4-32
export NUM_TPU_POOLS=2
export NAMESPACE=tpu-training

gcloud builds submit \
  --config cloudbuild.destroy.yaml \
  --substitutions _TF_STATE_BUCKET=$TF_STATE_BUCKET,_TF_STATE_PREFIX=$TF_STATE_PREFIX,_REGION=$REGION,_TENSORBOARD_REGION=$TENSORBOARD_REGION,_ZONE=$ZONE,_ARTIFACT_REPOSITORY_BUCKET_NAME=$ARTIFACT_REPOSITORY_BUCKET_NAME,_NETWORK_NAME=$NETWORK_NAME,_SUBNET_NAME=$SUBNET_NAME,_CLUSTER_NAME=$CLUSTER_NAME,_NAMESPACE=$NAMESPACE,_TPU_TYPE=$TPU_TYPE,_NUM_TPU_POOLS=$NUM_TPU_POOLS \
  --timeout "2h" \
  --machine-type=e2-highcpu-32 \
  --quiet

```



## Training workloads examples

The `examples` folder contains code samples that demonstrate how to configure, submit and manage a number of different training workloads. Refer to the README in this folder for detailed instructions.
